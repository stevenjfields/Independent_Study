{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a36f2c2f-7aa7-4fbd-92f1-b83161f4ab3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install transformers\n",
    "import pandas as pd\n",
    "from cogdl.oag import oagbert\n",
    "import torch\n",
    "import re\n",
    "import numpy as np\n",
    "import ipywidgets as widgets\n",
    "import requests\n",
    "import json\n",
    "from dataclasses import dataclass\n",
    "from typing import Dict, List, Optional\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "87037365-307e-4574-a86d-ae28217a6930",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "max_depth = 2\n",
    "ignore_related = False\n",
    "ignore_referenced = False\n",
    "base_works_url = \"https://api.openalex.org/works\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "95a67f80",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Article:\n",
    "    # Keeping track of some needed paper details\n",
    "    id: str\n",
    "    title: str\n",
    "    inverted_abstract: Dict[str, List[int]]\n",
    "    authors: List[str]\n",
    "    host_venue: str\n",
    "    affiliations: List[str]\n",
    "    concepts: List[str]\n",
    "    references: List[str]\n",
    "    related: List[str]\n",
    "\n",
    "    def get_abstract(self) -> str:\n",
    "        abstract = dict()\n",
    "        for k, v in self.inverted_abstract.items():\n",
    "            for i in v:\n",
    "                abstract[i] = k\n",
    "\n",
    "        final = \"\"\n",
    "        for i in sorted(abstract.keys()):\n",
    "            final += abstract[i] + \" \"\n",
    "        return final\n",
    "    \n",
    "    def fetch_references_queries(self):\n",
    "        # open alex only allows 50 OR joins per request\n",
    "        queries = list()\n",
    "        for i in range(0, len(self.references), 50):\n",
    "            queries.append('|'.join(self.references[i:i+50]))\n",
    "        return queries\n",
    "    \n",
    "    def fetch_related_queries(self):\n",
    "        # open alex only allows 50 OR joins per request\n",
    "        queries = list()\n",
    "        for i in range(0, len(self.related), 50):\n",
    "            queries.append('|'.join(self.related[i:i+50]))\n",
    "        return queries\n",
    "    \n",
    "    def __str__(self):\n",
    "        return f\"{self.id}: {self.title}\\n{self.get_abstract()}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d5213e84",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_article(result):\n",
    "    work_id = result[\"id\"].split('/')[-1]\n",
    "    title = result[\"title\"]\n",
    "    inverted_abstract = result['abstract_inverted_index']\n",
    "    authors = [authorship['author']['display_name'] for authorship in result['authorships']]\n",
    "    host_venue = result['host_venue']['publisher']\n",
    "    institutions = list()\n",
    "\n",
    "    for authorship in result['authorships']:\n",
    "        for institution in authorship['institutions']: \n",
    "            if institution['display_name'] not in institutions:\n",
    "                institutions.append(institution['display_name'])\n",
    "\n",
    "    concepts = [concept['display_name'] for concept in result['concepts'] if float(concept['score']) > 0.5]\n",
    "    referenced_works = [work.split('/')[-1] for work in result['referenced_works']]\n",
    "    related_works = [work.split('/')[-1] for work in result['related_works']]\n",
    "\n",
    "    return Article(\n",
    "        work_id,\n",
    "        title if title else \"\",\n",
    "        inverted_abstract if inverted_abstract else {\"\": [0]},\n",
    "        authors,\n",
    "        host_venue if host_venue else \"\",\n",
    "        institutions,\n",
    "        concepts,\n",
    "        referenced_works,\n",
    "        related_works\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e4acd78-69c6-4c12-b59f-7c23b14f07b5",
   "metadata": {},
   "source": [
    "# Search for Article Title\n",
    "Edit the title variable below to search for a paper. If not exact then returns 25 most relevant papers in the OpenAlex dataset. Select the paper in the dropdown menu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c333ad3f-a953-494e-a1de-1e3bde69c92b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bb72b108d58643018609ba2c67a6eff7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dropdown(description='Title: ', options=('Attention is All you Need', 'Attention Is All You Need', 'All That Gâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "title = \"Attention is all\"\n",
    "title = title.replace(\" \", \"%20\")\n",
    "req = requests.get(base_works_url+f\"?filter=title.search:{title}\")\n",
    "response = json.loads(req.content)\n",
    "\n",
    "relevant_titles = [result['title'] for result in response['results']]\n",
    "title_selector = widgets.Dropdown(\n",
    "    options=relevant_titles,\n",
    "    value=relevant_titles[0],\n",
    "    description=\"Title: \"\n",
    ")\n",
    "display(title_selector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d531b454",
   "metadata": {},
   "outputs": [
    {
     "ename": "Exception",
     "evalue": "Please select correct title above. If done, run all cells below this one.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mException\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mPlease select correct title above. If done, run all cells below this one.\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[0;31mException\u001b[0m: Please select correct title above. If done, run all cells below this one."
     ]
    }
   ],
   "source": [
    "raise Exception(\"Please select correct title above. If done, run all cells below this one.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3c0ca5f4-8341-4d93-be7b-685485140c04",
   "metadata": {},
   "outputs": [],
   "source": [
    "index = relevant_titles.index(title_selector.value)\n",
    "papers = dict()\n",
    "root_id = response['results'][index]['id'].split('/')[-1]\n",
    "\n",
    "papers[root_id] = fetch_article(response['results'][index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "84362a71",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Article(id='W2963403868', title='Attention is All you Need', inverted_abstract={'The': [0, 19], 'dominant': [1], 'sequence': [2], 'transduction': [3], 'models': [4, 23, 59], 'are': [5], 'based': [6, 41], 'on': [7, 52], 'complex': [8], 'recurrent': [9], 'orconvolutional': [10], 'neural': [11], 'networks': [12], 'in': [13], 'an': [14, 31], 'encoder': [15, 27], 'and': [16, 28, 49, 68], 'decoder': [17, 29], 'configuration.': [18], 'best': [20, 90], 'performing': [21], 'such': [22], 'also': [24], 'connect': [25], 'the': [26, 88, 102], 'through': [30], 'attentionm': [32], 'echanisms.': [33], 'We': [34], 'propose': [35], 'a': [36, 111], 'novel,': [37], 'simple': [38], 'network': [39], 'architecture': [40], 'solely': [42], 'onan': [43], 'attention': [44], 'mechanism,': [45], 'dispensing': [46], 'with': [47, 77, 105], 'recurrence': [48], 'convolutions': [50], 'entirely.Experiments': [51], 'two': [53], 'machine': [54], 'translation': [55], 'tasks': [56], 'show': [57], 'these': [58], 'to': [60], 'be': [61], 'superiorin': [62], 'quality': [63], 'while': [64], 'being': [65], 'more': [66], 'parallelizable': [67], 'requiring': [69], 'significantly': [70], 'less': [71], 'timeto': [72], 'train.': [73], 'Our': [74], 'single': [75], 'model': [76, 106], '165': [78], 'million': [79], 'parameters,': [80], 'achieves': [81], '27.5': [82], 'BLEU': [83, 112], 'onEnglish-to-German': [84], 'translation,': [85, 99], 'improving': [86], 'over': [87, 94], 'existing': [89], 'ensemble': [91], 'result': [92], 'by': [93, 107], '1': [95], 'BLEU.': [96], 'On': [97], 'English-to-French': [98], 'we': [100], 'outperform': [101], 'previoussingle': [103], 'state-of-the-art': [104], '0.7': [108], 'BLEU,': [109], 'achieving': [110], 'score': [113], 'of': [114], '41.1.': [115]}, authors=['Ashish Vaswani', 'Noam Shazeer', 'Niki Parmar', 'Jakob Uszkoreit', 'Llion Jones', 'Aidan N. Gomez', 'Lukasz Kaiser', 'Illia Polosukhin'], host_venue='', affiliations=['Google', 'University of Southern California'], concepts=['Machine translation', 'BLEU', 'Computer science', 'Encoder', 'Parallelizable manifold', 'Artificial intelligence', 'Translation (biology)'], references=[], related=['W1902237438', 'W2064675550', 'W2095705004', 'W2101105183', 'W2108598243', 'W2130942839', 'W2153579005', 'W2154652894', 'W2157331557', 'W2163605009', 'W2194775991', 'W2250539671', 'W2525778437', 'W2962739339', 'W2962784628', 'W2963341956', 'W2964121744', 'W2964308564', 'W2965373594', 'W2970597249'])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "papers['W2963403868']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "60843a41",
   "metadata": {},
   "outputs": [],
   "source": [
    "use_references = ignore_referenced != True\n",
    "use_related = ignore_related != True\n",
    "\n",
    "related_works: Dict[int, List[Article]] = {}\n",
    "\n",
    "def get_relevant_papers(current_depth: int, previous: List[Article]):\n",
    "    related_works[current_depth] = []\n",
    "    print(current_depth)\n",
    "    for parent in previous:\n",
    "        if use_references and len(parent.references) > 0:\n",
    "            for query in parent.fetch_references_queries():         \n",
    "                req = requests.get(base_works_url + f'?filter=openalex_id:{query}')\n",
    "                res = json.loads(req.content)\n",
    "                for result in res[\"results\"]:\n",
    "                    paper_id = result['id'].split('/')[-1]\n",
    "                    if paper_id not in papers.keys():\n",
    "                        temp = fetch_article(result)\n",
    "                        papers[temp.id] = temp\n",
    "                        related_works[current_depth].append(temp)\n",
    "            \n",
    "        if use_related and len(parent.related) > 0:\n",
    "            for query in parent.fetch_related_queries():  \n",
    "                req = requests.get(base_works_url + f'?filter=openalex_id:{query}')\n",
    "                res = json.loads(req.content)\n",
    "                for result in res[\"results\"]:\n",
    "                    paper_id = result['id'].split('/')[-1]\n",
    "                    if paper_id not in papers.keys():\n",
    "                        temp = fetch_article(result)\n",
    "                        papers[temp.id] = temp\n",
    "                        related_works[current_depth].append(temp)\n",
    "\n",
    "    if current_depth < max_depth:\n",
    "        get_relevant_papers(current_depth+1, related_works[current_depth])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a10c6066",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "get_relevant_papers(1, [papers[root_id]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d10b4ac6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "422\n",
      "dict_keys(['W2963403868', 'W2194775991', 'W2064675550', 'W2108598243', 'W2964121744', 'W2250539671', 'W2963341956', 'W2163605009', 'W2101105183', 'W2157331557', 'W2964308564', 'W2095705004', 'W2153579005', 'W2962739339', 'W2130942839', 'W1902237438', 'W2965373594', 'W2962784628', 'W2154652894', 'W2525778437', 'W2970597249', 'W2097117768', 'W1903029394', 'W2102605133', 'W1536680647', 'W2031489346', 'W1677182931', 'W2147800946', 'W2107878631', 'W2117812871', 'W2124509324', 'W2147238549', 'W1984309565', 'W1997542937', 'W1932847118', 'W1976921161', 'W4242212377', 'W4238404964', 'W2159979951', 'W2964103341', 'W4231990273', 'W4212915314', 'W2518214538', 'W3099850646', 'W2005051400', 'W2733060750', 'W4245792239', 'W2743258233', 'W2972435282', 'W3178127657', 'W2773120646', 'W3108696707', 'W2007431958', 'W1971129545', 'W2143503258', 'W2103452139', 'W2154890045', 'W2048060899', 'W2123716044', 'W2121553911', 'W2036317923', 'W1984375561', 'W2156960699', 'W2086756055', 'W2114471683', 'W1982291194', 'W2165432985', 'W1971209221', 'W1963605035', 'W2787045460', 'W3005641657', 'W3156786002', 'W3011883770', 'W2182757990', 'W2362189222', 'W2391384657', 'W3135485313', 'W3176779650', 'W4220769644', 'W2151103935', 'W2128017662', 'W2110764733', 'W2115733720', 'W1997011019', 'W2141282920', 'W2145607950', 'W2166742463', 'W2172191903', 'W2149489787', 'W2106097867', 'W2134135198', 'W2158535772', 'W1521539493', 'W2009606665', 'W2150499647', 'W2326857978', 'W1560761292', 'W1485554661', 'W2142918948', 'W1597238586', 'W1601975371', 'W1851156223', 'W2376314740', 'W2905461407', 'W1977145549', 'W2116005283', 'W2160374150', 'W2075181955', 'W2026733907', 'W2493118020', 'W2969212598', 'W1991827598', 'W1849009482', 'W3119285795', 'W2147152072', 'W2118020653', 'W2117130368', 'W2146502635', 'W2132339004', 'W2158899491', 'W1614298861', 'W2072128103', 'W2144578941', 'W1495550473', 'W2141599568', 'W1981617416', 'W2103318667', 'W2080100102', 'W2158139315', 'W2251803266', 'W2067438047', 'W2164019165', 'W1978400666', 'W2251012068', 'W2133280805', 'W2250189634', 'W2077428231', 'W2167510172', 'W2151447942', 'W3175170601', 'W1508636238', 'W2985678088', 'W3107474891', 'W2360025963', 'W2360785147', 'W2995335655', 'W3095760691', 'W4287629333', 'W2025768430', 'W2963748441', 'W2251939518', 'W1840435438', 'W2963026768', 'W2963846996', 'W2121227244', 'W2270070752', 'W2131744502', 'W2963918774', 'W2158108973', 'W2149933564', 'W1566289585', 'W2413794162', 'W2963339397', 'W2130903752', 'W2551396370', 'W3104033643', 'W3098057198', 'W2971274815', 'W2945260553', 'W2970202851', 'W3023676446', 'W1542956019', 'W2960591372', 'W4221149706', 'W4226242668', 'W4226315315', 'W2911964244', 'W3118608800', 'W1665214252', 'W1904365287', 'W2130325614', 'W2166049352', 'W2546302380', 'W2156163116', 'W2015861736', 'W2134557905', 'W2154579312', 'W1576445103', 'W2108069432', 'W2061212083', 'W2026942141', 'W2169805405', 'W2144161366', 'W2053229256', 'W1499991161', 'W2396976214', 'W2097356275', 'W2919115771', 'W2112796928', 'W2117539524', 'W1901129140', 'W2963446712', 'W1686810756', 'W2183341477', 'W2155893237', 'W2949117887', 'W1849277567', 'W2613718673', 'W3037252522', 'W2001810881', 'W2732923061', 'W11531451', 'W2900389025', 'W2611614995', 'W2789919619', 'W3169305685', 'W1552159754', 'W2293457016', 'W2368651715', 'W2153653739', 'W2147768505', 'W6908809', 'W22168010', 'W2156387975', 'W2144499799', 'W1753482797', 'W1606347560', 'W2103305545', 'W1970689298', 'W2161792612', 'W2251682575', 'W2117278770', 'W1905522558', 'W2118090838', 'W2108563286', 'W2963504252', 'W3037950864', 'W2964335273', 'W932413789', 'W2250489405', 'W2950635152', 'W2116792345', 'W2807006873', 'W3166311385', 'W2097033888', 'W2251771687', 'W2896411932', 'W2188746779', 'W2906854570', 'W2131774270', 'W2964199361', 'W1810943226', 'W1815076433', 'W2005708641', 'W1916559533', 'W1828163288', 'W2294059674', 'W194249466', 'W2103078213', 'W2964222437', 'W2251222643', 'W2395935897', 'W2964335437', 'W2950577311', 'W1924770834', 'W1514535095', 'W2606974598', 'W2135046866', 'W2100495367', 'W2136922672', 'W1524333225', 'W1567512734', 'W2335728318', 'W2145094598', 'W1993882792', 'W2131241448', 'W2085040216', 'W189596042', 'W2156297475', 'W2114296159', 'W1492459858', 'W2971788173', 'W2949821452', 'W35527955', 'W1533861849', 'W1498436455', 'W1662133657', 'W2171928131', 'W1889268436', 'W22861983', 'W1423339008', 'W2131462252', 'W36903255', 'W1965154800', 'W21006490', 'W2138204974', 'W2120861206', 'W2962769333', 'W1880262756', 'W1832693441', 'W2187089797', 'W2962756421', 'W2493916176', 'W3104097132', 'W2147880316', 'W1632114991', 'W2296283641', 'W2158847908', 'W2963625095', 'W1938755728', 'W2963266340', 'W2608787653', 'W2259472270', 'W2740747242', 'W581956982', 'W2155069789', 'W2964222246', 'W2964091467', 'W3016169217', 'W2148757832', 'W2150355110', 'W2127141656', 'W179875071', 'W2141125852', 'W2402268235', 'W2184045248', 'W1525783482', 'W2253807446', 'W2136939460', 'W2143612262', 'W1895577753', 'W2950178297', 'W1591801644', 'W2100664567', 'W2118434577', 'W2147527908', 'W2169724380', 'W1586532344', 'W2962741254', 'W2086202918', 'W2250653840', 'W1544285860', 'W2961085424', 'W2098551021', 'W2435130738', 'W2245229737', 'W92653481', 'W4210841218', 'W4224009465', 'W4237498193', 'W2899771611', 'W2963310665', 'W2933138175', 'W2525127255', 'W2963323070', 'W2130158090', 'W131533222', 'W2990704537', 'W1599016936', 'W2396767181', 'W2944815030', 'W2963756346', 'W2899663614', 'W2462831000', 'W4280535922', 'W88325386', 'W4210794429', 'W1563618553', 'W3014815208', 'W3199608561', 'W4223456145', 'W4283697347', 'W2124807415', 'W2148708890', 'W1899794420', 'W2250342921', 'W46679369', 'W2117621558', 'W2016856586', 'W2220350356', 'W1501139663', 'W2116211107', 'W1505680913', 'W2116599427', 'W2119202242', 'W2015350341', 'W2121524931', 'W626175318', 'W2134352815', 'W2167662847', 'W1512718085', 'W193726211', 'W2175555681', 'W2377856297', 'W2610387714', 'W3145128584', 'W2752885492', 'W2097879961', 'W2150824314', 'W3151369355', 'W2108325777', 'W2088781183', 'W2963010813', 'W2112174218', 'W2117010802', 'W1493638625', 'W1974339500', 'W1956340063', 'W2123301721', 'W2962965405', 'W1525595230', 'W1544827683', 'W2963929190', 'W3101913037', 'W2121879602', 'W2251743902', 'W1968594024', 'W2964265128', 'W3082274269', 'W2996428491', 'W2964110616', 'W2978017171'])\n"
     ]
    }
   ],
   "source": [
    "print(len(papers.keys()))\n",
    "print(papers.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "66b7dde6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W4210794429: On Automating Hyperparameter Optimization for Deep Learning Applications\n",
      "Given a large amount of data and appropriate hyperparameters, deep learning techniques can deliver impressive performance if several challenging issues with training, such as vanishing gradients, can be overcome. Often, deep learning training techniques produce suboptimal results because the parameter search space is large and populated with many less-than-ideal solutions. Automatic hyperparameter tuning algorithms, known as autotuners, offer an attractive alternative for automating the training process, though they can be computationally expensive. Additionally, autotuners democratize state-of-the-art machine learning approaches and increase the accessibility of deep learning technology to different scientific communities and novice users. In this paper, we investigate the efficacy of autotuning using Keras Tuner on both synthetic and real-world datasets. We show that autotuning performed well on synthetic datasets but was inadequate on real data. As we increase model complexity, autotuning produces errors that are tedious to resolve for those with limited experience in machine learning. Avoiding overfitting, for example, requires extensive knowledge of an algorithm's unique characteristics (e.g., adding dropout layers). Autotuning tools are excellent for creating baseline models on new datasets, but they need more attention to formulate optimal solutions for end-users with less background in deep learning. Because of this, manual tuning based on domain knowledge and experience is still preferred in machine learning because it produces better performance, even though it requires extensive machine learning expertise. \n"
     ]
    }
   ],
   "source": [
    "print(papers['W4210794429'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c9f04e0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer, model = oagbert(\"oagbert-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ee4469d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(\"./embeddings/\"):\n",
    "    os.mkdir(\"./embeddings/\")\n",
    "\n",
    "files = os.listdir(\"./embeddings/\")\n",
    "files = [file.split('.')[0] for file in files]\n",
    "\n",
    "for key in papers.keys():\n",
    "    if key not in files:\n",
    "        curr_paper = papers[key]\n",
    "        input_ids, input_masks, token_type_ids, masked_lm_labels, position_ids, position_ids_second, masked_positions, num_spans = model.build_inputs(\n",
    "            title=curr_paper.title, \n",
    "            abstract=curr_paper.get_abstract(), \n",
    "            venue=curr_paper.host_venue, \n",
    "            authors=curr_paper.authors, \n",
    "            concepts=curr_paper.concepts, \n",
    "            affiliations=curr_paper.affiliations\n",
    "        )\n",
    "\n",
    "        sequence_output, pooled_output = model.bert.forward(\n",
    "            input_ids=torch.LongTensor(input_ids).unsqueeze(0),\n",
    "            token_type_ids=torch.LongTensor(token_type_ids).unsqueeze(0),\n",
    "            attention_mask=torch.LongTensor(input_masks).unsqueeze(0),\n",
    "            output_all_encoded_layers=False,\n",
    "            checkpoint_activations=False,\n",
    "            position_ids=torch.LongTensor(position_ids).unsqueeze(0),\n",
    "            position_ids_second=torch.LongTensor(position_ids_second).unsqueeze(0)\n",
    "        )\n",
    "\n",
    "        torch.save(pooled_output, f\"./embeddings/{curr_paper.id}.pt\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6af4ff49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "422\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "421"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "root_paper_embeddings = torch.load(f\"./embeddings/{root_id}.pt\")\n",
    "comparison_batch_size = 25\n",
    "\n",
    "paper_keys = list(papers.keys())\n",
    "paper_keys.remove(root_id)\n",
    "\n",
    "for i in range(0, len(paper_keys), 25):\n",
    "    key_slice = paper_keys[i:i+25]\n",
    "    for key in key_slice:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e052225f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "ad6f02c4897e5a2f50ed3c5fec7e665ad608d4dc358906e6dd46ab054316280e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
