{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a36f2c2f-7aa7-4fbd-92f1-b83161f4ab3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install transformers\n",
    "import pandas as pd\n",
    "from cogdl.oag import oagbert\n",
    "import torch\n",
    "import re\n",
    "import numpy as np\n",
    "import ipywidgets as widgets\n",
    "import requests\n",
    "import json\n",
    "from dataclasses import dataclass\n",
    "from typing import Dict, List, Optional\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "87037365-307e-4574-a86d-ae28217a6930",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "max_depth = 2\n",
    "ignore_related = False\n",
    "ignore_referenced = False\n",
    "base_works_url = \"https://api.openalex.org/works\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "95a67f80",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Article:\n",
    "    # Keeping track of some needed paper details\n",
    "    id: str\n",
    "    title: str\n",
    "    inverted_abstract: Dict[str, List[int]]\n",
    "    authors: List[str]\n",
    "    host_venue: str\n",
    "    affiliations: List[str]\n",
    "    concepts: List[str]\n",
    "    references: List[str]\n",
    "    related: List[str]\n",
    "\n",
    "    def get_abstract(self) -> str:\n",
    "        abstract = dict()\n",
    "        for k, v in self.inverted_abstract.items():\n",
    "            for i in v:\n",
    "                abstract[i] = k\n",
    "\n",
    "        final = \"\"\n",
    "        for i in sorted(abstract.keys()):\n",
    "            final += abstract[i] + \" \"\n",
    "        return final\n",
    "    \n",
    "    def fetch_references_queries(self):\n",
    "        # open alex only allows 50 OR joins per request\n",
    "        queries = list()\n",
    "        for i in range(0, len(self.references), 50):\n",
    "            queries.append('|'.join(self.references[i:i+50]))\n",
    "        return queries\n",
    "    \n",
    "    def fetch_related_queries(self):\n",
    "        # open alex only allows 50 OR joins per request\n",
    "        queries = list()\n",
    "        for i in range(0, len(self.related), 50):\n",
    "            queries.append('|'.join(self.related[i:i+50]))\n",
    "        return queries\n",
    "    \n",
    "    def __str__(self):\n",
    "        return f\"{self.id}: {self.title}\\n{self.get_abstract()}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d5213e84",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_article(result):\n",
    "    work_id = result[\"id\"].split('/')[-1]\n",
    "    title = result[\"title\"]\n",
    "    inverted_abstract = result['abstract_inverted_index']\n",
    "    authors = [authorship['author']['display_name'] for authorship in result['authorships']]\n",
    "    host_venue = result['host_venue']['publisher']\n",
    "    institutions = list()\n",
    "\n",
    "    for authorship in result['authorships']:\n",
    "        for institution in authorship['institutions']: \n",
    "            if institution['display_name'] not in institutions:\n",
    "                institutions.append(institution['display_name'])\n",
    "\n",
    "    concepts = [concept['display_name'] for concept in result['concepts'] if float(concept['score']) > 0.5]\n",
    "    referenced_works = [work.split('/')[-1] for work in result['referenced_works']]\n",
    "    related_works = [work.split('/')[-1] for work in result['related_works']]\n",
    "\n",
    "    return Article(\n",
    "        work_id,\n",
    "        title if title else \"\",\n",
    "        inverted_abstract if inverted_abstract else {\"\": [0]},\n",
    "        authors,\n",
    "        host_venue if host_venue else \"\",\n",
    "        institutions,\n",
    "        concepts,\n",
    "        referenced_works,\n",
    "        related_works\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e4acd78-69c6-4c12-b59f-7c23b14f07b5",
   "metadata": {},
   "source": [
    "# Search for Article Title\n",
    "Edit the title variable below to search for a paper. If not exact then returns 25 most relevant papers in the OpenAlex dataset. Select the paper in the dropdown menu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "c333ad3f-a953-494e-a1de-1e3bde69c92b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "88b35ca7ba51491bbb436e820b768972",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dropdown(description='Title: ', options=('YOLOv4: Optimal Speed and Accuracy of Object Detection', 'Scaled-YOLâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "title = \"Yolov4\"\n",
    "title = title.replace(\" \", \"%20\")\n",
    "req = requests.get(base_works_url+f\"?filter=title.search:{title}\")\n",
    "response = json.loads(req.content)\n",
    "\n",
    "relevant_titles = [result['title'] for result in response['results']]\n",
    "title_selector = widgets.Dropdown(\n",
    "    options=relevant_titles,\n",
    "    value=relevant_titles[0],\n",
    "    description=\"Title: \"\n",
    ")\n",
    "display(title_selector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d531b454",
   "metadata": {},
   "outputs": [
    {
     "ename": "Exception",
     "evalue": "Please select correct title above. If done, run all cells below this one.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mException\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mPlease select correct title above. If done, run all cells below this one.\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[0;31mException\u001b[0m: Please select correct title above. If done, run all cells below this one."
     ]
    }
   ],
   "source": [
    "raise Exception(\"Please select correct title above. If done, run all cells below this one.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "3c0ca5f4-8341-4d93-be7b-685485140c04",
   "metadata": {},
   "outputs": [],
   "source": [
    "index = relevant_titles.index(title_selector.value)\n",
    "papers = dict()\n",
    "root_id = response['results'][index]['id'].split('/')[-1]\n",
    "\n",
    "papers[root_id] = fetch_article(response['results'][index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "60843a41",
   "metadata": {},
   "outputs": [],
   "source": [
    "use_references = ignore_referenced != True\n",
    "use_related = ignore_related != True\n",
    "\n",
    "related_works: Dict[int, List[Article]] = {}\n",
    "\n",
    "def get_relevant_papers(current_depth: int, previous: List[Article]):\n",
    "    related_works[current_depth] = []\n",
    "    print(current_depth)\n",
    "    for parent in previous:\n",
    "        if use_references and len(parent.references) > 0:\n",
    "            for query in parent.fetch_references_queries():         \n",
    "                req = requests.get(base_works_url + f'?filter=openalex_id:{query}')\n",
    "                res = json.loads(req.content)\n",
    "                for result in res[\"results\"]:\n",
    "                    paper_id = result['id'].split('/')[-1]\n",
    "                    if paper_id not in papers.keys():\n",
    "                        temp = fetch_article(result)\n",
    "                        papers[temp.id] = temp\n",
    "                        related_works[current_depth].append(temp)\n",
    "            \n",
    "        if use_related and len(parent.related) > 0:\n",
    "            for query in parent.fetch_related_queries():  \n",
    "                req = requests.get(base_works_url + f'?filter=openalex_id:{query}')\n",
    "                res = json.loads(req.content)\n",
    "                for result in res[\"results\"]:\n",
    "                    paper_id = result['id'].split('/')[-1]\n",
    "                    if paper_id not in papers.keys():\n",
    "                        temp = fetch_article(result)\n",
    "                        papers[temp.id] = temp\n",
    "                        related_works[current_depth].append(temp)\n",
    "\n",
    "    if current_depth < max_depth:\n",
    "        get_relevant_papers(current_depth+1, related_works[current_depth])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "a10c6066",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "get_relevant_papers(1, [papers[root_id]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "c9f04e0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer, model = oagbert(\"oagbert-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "ee4469d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(\"./embeddings/\"):\n",
    "    os.mkdir(\"./embeddings/\")\n",
    "\n",
    "files = os.listdir(\"./embeddings/\")\n",
    "files = [file.split('.')[0] for file in files]\n",
    "\n",
    "for key in papers.keys():\n",
    "    if key not in files:\n",
    "        curr_paper = papers[key]\n",
    "        input_ids, input_masks, token_type_ids, masked_lm_labels, position_ids, position_ids_second, masked_positions, num_spans = model.build_inputs(\n",
    "            title=curr_paper.title, \n",
    "            abstract=curr_paper.get_abstract(), \n",
    "            venue=curr_paper.host_venue, \n",
    "            authors=curr_paper.authors, \n",
    "            concepts=curr_paper.concepts, \n",
    "            affiliations=curr_paper.affiliations\n",
    "        )\n",
    "\n",
    "        sequence_output, pooled_output = model.bert.forward(\n",
    "            input_ids=torch.LongTensor(input_ids).unsqueeze(0),\n",
    "            token_type_ids=torch.LongTensor(token_type_ids).unsqueeze(0),\n",
    "            attention_mask=torch.LongTensor(input_masks).unsqueeze(0),\n",
    "            output_all_encoded_layers=False,\n",
    "            checkpoint_activations=False,\n",
    "            position_ids=torch.LongTensor(position_ids).unsqueeze(0),\n",
    "            position_ids_second=torch.LongTensor(position_ids_second).unsqueeze(0)\n",
    "        )\n",
    "\n",
    "        torch.save(pooled_output, f\"./embeddings/{curr_paper.id}.pt\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "6af4ff49",
   "metadata": {},
   "outputs": [],
   "source": [
    "root_paper_embeddings = torch.load(f\"./embeddings/{root_id}.pt\")\n",
    "root_paper_embeddings = torch.nn.functional.normalize(root_paper_embeddings, p=2, dim=1)\n",
    "\n",
    "paper_keys = list(papers.keys())\n",
    "paper_keys.remove(root_id)\n",
    "\n",
    "cols = [\"id\", \"title\", \"score\"]\n",
    "similarities = pd.DataFrame(columns=cols)\n",
    "\n",
    "for key in paper_keys:\n",
    "    paper_embeddings = torch.load(f\"./embeddings/{key}.pt\")\n",
    "    paper_embeddings = torch.nn.functional.normalize(paper_embeddings, p=2, dim=1)\n",
    "    sim = torch.mm(root_paper_embeddings, paper_embeddings.transpose(0, 1))\n",
    "    results = {\n",
    "        \"id\": [key],\n",
    "        \"title\": [papers[key].title],\n",
    "        \"score\": [sim.detach().numpy()]\n",
    "    }\n",
    "\n",
    "    similarities = pd.concat([similarities, pd.DataFrame(results)], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "e052225f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>435</th>\n",
       "      <td>W2550553598</td>\n",
       "      <td>SCA-CNN: Spatial and Channel-Wise Attention in...</td>\n",
       "      <td>[[0.9953112]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>839</th>\n",
       "      <td>W2113201641</td>\n",
       "      <td>Beyond sliding windows: Object localization by...</td>\n",
       "      <td>[[0.99525994]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>506</th>\n",
       "      <td>W2949605076</td>\n",
       "      <td>Rethinking the Inception Architecture for Comp...</td>\n",
       "      <td>[[0.9950305]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>W2109255472</td>\n",
       "      <td>Spatial Pyramid Pooling in Deep Convolutional ...</td>\n",
       "      <td>[[0.99494416]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>777</th>\n",
       "      <td>W4214742200</td>\n",
       "      <td>RAiA-Net: A Multi-Stage Network With Refined A...</td>\n",
       "      <td>[[0.9943474]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>732</th>\n",
       "      <td>W2963584620</td>\n",
       "      <td>StairNet: Top-Down Semantic Aggregation for Ac...</td>\n",
       "      <td>[[0.99431604]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>720</th>\n",
       "      <td>W2981392049</td>\n",
       "      <td>Enriching Variety of Layer-Wise Learning Infor...</td>\n",
       "      <td>[[0.99429107]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>489</th>\n",
       "      <td>W2572745118</td>\n",
       "      <td>Beyond Skip Connections: Top-Down Modulation f...</td>\n",
       "      <td>[[0.9942732]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>W2963037989</td>\n",
       "      <td>You Only Look Once: Unified, Real-Time Object ...</td>\n",
       "      <td>[[0.99422127]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>206</th>\n",
       "      <td>W1944396096</td>\n",
       "      <td>Ontological supervision for fine grained class...</td>\n",
       "      <td>[[0.99412495]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>160</th>\n",
       "      <td>W2141200610</td>\n",
       "      <td>Adaptive deconvolutional networks for mid and ...</td>\n",
       "      <td>[[0.9941225]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>255</th>\n",
       "      <td>W2179352600</td>\n",
       "      <td>Spatial Pyramid Pooling in Deep Convolutional ...</td>\n",
       "      <td>[[0.9941122]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>683</th>\n",
       "      <td>W2200178803</td>\n",
       "      <td>Spatial Semantic Regularisation for Large Scal...</td>\n",
       "      <td>[[0.9940751]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>273</th>\n",
       "      <td>W2125215748</td>\n",
       "      <td>The Role of Context for Object Detection and S...</td>\n",
       "      <td>[[0.9940616]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>470</th>\n",
       "      <td>W2104636679</td>\n",
       "      <td>Accelerating Very Deep Convolutional Networks ...</td>\n",
       "      <td>[[0.9940574]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>W2166742463</td>\n",
       "      <td>In defense of Nearest-Neighbor based image cla...</td>\n",
       "      <td>[[0.994029]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>802</th>\n",
       "      <td>W2150066425</td>\n",
       "      <td>Are we ready for autonomous driving? The KITTI...</td>\n",
       "      <td>[[0.99398774]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>333</th>\n",
       "      <td>W2963403868</td>\n",
       "      <td>Attention is All you Need</td>\n",
       "      <td>[[0.99391615]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>365</th>\n",
       "      <td>W2103658758</td>\n",
       "      <td>Discovering objects and their location in images</td>\n",
       "      <td>[[0.993914]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>W2949117887</td>\n",
       "      <td>Batch Normalization: Accelerating Deep Network...</td>\n",
       "      <td>[[0.99379706]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>W1677182931</td>\n",
       "      <td>Delving Deep into Rectifiers: Surpassing Human...</td>\n",
       "      <td>[[0.9937766]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>403</th>\n",
       "      <td>W2147414309</td>\n",
       "      <td>PANDA: Pose Aligned Networks for Deep Attribut...</td>\n",
       "      <td>[[0.9937605]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>143</th>\n",
       "      <td>W2952186574</td>\n",
       "      <td>Visualizing and Understanding Convolutional Ne...</td>\n",
       "      <td>[[0.993743]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>415</th>\n",
       "      <td>W2952432176</td>\n",
       "      <td>Compressing Neural Networks with the Hashing T...</td>\n",
       "      <td>[[0.99373937]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>492</th>\n",
       "      <td>W2407521645</td>\n",
       "      <td>R-FCN: Object Detection via Region-based Fully...</td>\n",
       "      <td>[[0.99373126]]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              id                                              title  \\\n",
       "435  W2550553598  SCA-CNN: Spatial and Channel-Wise Attention in...   \n",
       "839  W2113201641  Beyond sliding windows: Object localization by...   \n",
       "506  W2949605076  Rethinking the Inception Architecture for Comp...   \n",
       "18   W2109255472  Spatial Pyramid Pooling in Deep Convolutional ...   \n",
       "777  W4214742200  RAiA-Net: A Multi-Stage Network With Refined A...   \n",
       "732  W2963584620  StairNet: Top-Down Semantic Aggregation for Ac...   \n",
       "720  W2981392049  Enriching Variety of Layer-Wise Learning Infor...   \n",
       "489  W2572745118  Beyond Skip Connections: Top-Down Modulation f...   \n",
       "5    W2963037989  You Only Look Once: Unified, Real-Time Object ...   \n",
       "206  W1944396096  Ontological supervision for fine grained class...   \n",
       "160  W2141200610  Adaptive deconvolutional networks for mid and ...   \n",
       "255  W2179352600  Spatial Pyramid Pooling in Deep Convolutional ...   \n",
       "683  W2200178803  Spatial Semantic Regularisation for Large Scal...   \n",
       "273  W2125215748  The Role of Context for Object Detection and S...   \n",
       "470  W2104636679  Accelerating Very Deep Convolutional Networks ...   \n",
       "88   W2166742463  In defense of Nearest-Neighbor based image cla...   \n",
       "802  W2150066425  Are we ready for autonomous driving? The KITTI...   \n",
       "333  W2963403868                          Attention is All you Need   \n",
       "365  W2103658758   Discovering objects and their location in images   \n",
       "13   W2949117887  Batch Normalization: Accelerating Deep Network...   \n",
       "9    W1677182931  Delving Deep into Rectifiers: Surpassing Human...   \n",
       "403  W2147414309  PANDA: Pose Aligned Networks for Deep Attribut...   \n",
       "143  W2952186574  Visualizing and Understanding Convolutional Ne...   \n",
       "415  W2952432176  Compressing Neural Networks with the Hashing T...   \n",
       "492  W2407521645  R-FCN: Object Detection via Region-based Fully...   \n",
       "\n",
       "              score  \n",
       "435   [[0.9953112]]  \n",
       "839  [[0.99525994]]  \n",
       "506   [[0.9950305]]  \n",
       "18   [[0.99494416]]  \n",
       "777   [[0.9943474]]  \n",
       "732  [[0.99431604]]  \n",
       "720  [[0.99429107]]  \n",
       "489   [[0.9942732]]  \n",
       "5    [[0.99422127]]  \n",
       "206  [[0.99412495]]  \n",
       "160   [[0.9941225]]  \n",
       "255   [[0.9941122]]  \n",
       "683   [[0.9940751]]  \n",
       "273   [[0.9940616]]  \n",
       "470   [[0.9940574]]  \n",
       "88     [[0.994029]]  \n",
       "802  [[0.99398774]]  \n",
       "333  [[0.99391615]]  \n",
       "365    [[0.993914]]  \n",
       "13   [[0.99379706]]  \n",
       "9     [[0.9937766]]  \n",
       "403   [[0.9937605]]  \n",
       "143    [[0.993743]]  \n",
       "415  [[0.99373937]]  \n",
       "492  [[0.99373126]]  "
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "similarities.sort_values(by=\"score\", ascending=False).head(25)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "da0f4927",
   "metadata": {},
   "source": [
    "# Print Root Paper Abstract and 2 most similar papers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "f80b726d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are a huge number of features which are said to improve Convolutional Neural Network (CNN) accuracy. Practical testing of combinations of such features on large datasets, and theoretical justification of the result, is required. Some features operate on certain models exclusively and for certain problems exclusively, or only for small-scale datasets; while some features, such as batch-normalization and residual-connections, are applicable to the majority of models, tasks, and datasets. We assume that such universal features include Weighted-Residual-Connections (WRC), Cross-Stage-Partial-connections (CSP), Cross mini-Batch Normalization (CmBN), Self-adversarial-training (SAT) and Mish-activation. We use new features: WRC, CSP, CmBN, SAT, Mish activation, Mosaic data augmentation, CmBN, DropBlock regularization, and CIoU loss, and combine some of them to achieve state-of-the-art results: 43.5% AP (65.7% AP50) for the MS COCO dataset at a realtime speed of ~65 FPS on Tesla V100. Source code is at this https URL \n"
     ]
    }
   ],
   "source": [
    "print(papers[root_id].get_abstract())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "42ff32c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Visual attention has been successfully applied in structural prediction tasks such as visual captioning and question answering. Existing visual attention models are generally spatial, i.e., the attention is modeled as spatial probabilities that re-weight the last conv-layer feature map of a CNN encoding an input image. However, we argue that such spatial attention does not necessarily conform to the attention mechanism --- a dynamic feature extractor that combines contextual fixations over time, as CNN features are naturally spatial, channel-wise and multi-layer. In this paper, we introduce a novel convolutional neural network dubbed SCA-CNN that incorporates Spatial and Channel-wise Attentions in a CNN. In the task of image captioning, SCA-CNN dynamically modulates the sentence generation context in multi-layer feature maps, encoding where (i.e., attentive spatial locations at multiple layers) and what (i.e., attentive channels) the visual attention is. We evaluate the proposed SCA-CNN architecture on three benchmark image captioning datasets: Flickr8K, Flickr30K, and MSCOCO. It is consistently observed that SCA-CNN significantly outperforms state-of-the-art visual attention-based image captioning methods. \n",
      "\n",
      "\n",
      "\n",
      "Most successful object recognition systems rely on binary classification, deciding only if an object is present or not, but not providing information on the actual object location. To perform localization, one can take a sliding window approach, but this strongly increases the computational cost, because the classifier function has to be evaluated over a large set of candidate subwindows. In this paper, we propose a simple yet powerful branch-and-bound scheme that allows efficient maximization of a large class of classifier functions over all possible subimages. It converges to a globally optimal solution typically in sublinear time. We show how our method is applicable to different object detection and retrieval scenarios. The achieved speedup allows the use of classifiers for localization that formerly were considered too slow for this task, such as SVMs with a spatial pyramid kernel or nearest neighbor classifiers based on the chi2-distance. We demonstrate state-of-the-art performance of the resulting systems on the UIUC Cars dataset, the PASCAL VOC 2006 dataset and in the PASCAL VOC 2007 competition. \n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ids = similarities.sort_values(by=\"score\", ascending=False).head(2)[\"id\"]\n",
    "for id in ids.values:\n",
    "    print(papers[id].get_abstract())\n",
    "    print(\"\\n\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "ad6f02c4897e5a2f50ed3c5fec7e665ad608d4dc358906e6dd46ab054316280e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
